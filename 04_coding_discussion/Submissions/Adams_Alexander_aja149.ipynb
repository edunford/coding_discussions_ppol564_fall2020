{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages and load bokeh module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in relevant text and csv files, and convert the stop words to a list\n",
    "aljz = open('../Data/aljazeera-khashoggi.txt', 'r', encoding = 'UTF-8').read()\n",
    "bbc = open('../Data/bbc-khashoggi.txt', 'r', encoding = 'UTF-8').read()\n",
    "bbt = open('../Data/breitbart-khashoggi.txt', 'r', encoding = 'UTF-8').read()\n",
    "cnn = open('../Data/cnn-khashoggi.txt', 'r', encoding = 'UTF-8').read()\n",
    "fox = open('../Data/fox-khashoggi.txt', 'r', encoding = 'UTF-8').read()\n",
    "stop_words = pd.read_csv('../Data/stop_words.csv')\n",
    "stop_words = stop_words['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to use a lot of functions from the trigonometry-of-vectors workbook\n",
    "#First, a function to tokenize each text file\n",
    "def tokenize(text=None):\n",
    "    \n",
    "    '''\n",
    "    Converts a long string into all lowercase, removes punctuation, and spits the string into individual substrings.\n",
    "    Removes any specified stop words. Then returns a list of substrings.\n",
    "    Arguments: text = A string or text\n",
    "    '''\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace('.','')\n",
    "    text_list = text.split()\n",
    "    text_list2 = [word for word in text_list if word not in stop_words]\n",
    "    return text_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, create a list of files, tokenize each, convert each file into a dictionary, and then into a document term matrix\n",
    "articles_tokenized = [aljz, bbc, bbt, cnn, fox]\n",
    "\n",
    "def convert_text_to_dtm(text):\n",
    "\n",
    "    '''\n",
    "    Converts a string or list of strings into a document term matrix (DTM), where column names are substrings \n",
    "    and column values are substring frequencies. Then returns the DTM as a pandas data frame.\n",
    "    Arguments: text = A string or text\n",
    "    '''\n",
    "    \n",
    "    d = dict()\n",
    "    for word in tokenize(text):\n",
    "        if word in d:\n",
    "            d[word][0] += 1\n",
    "        else:\n",
    "            d[word] = [1]\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "def gen_DTM(texts=None):\n",
    "    \n",
    "    '''\n",
    "    Converts each string in a list of strings into a document term matrix (DTM), then appends it to a larger matrix.\n",
    "    Fills in any missing values with 0, and returns the DTM as a multi-row pandas data frame.\n",
    "    Arguments: texts = A list of strings or texts\n",
    "    '''\n",
    "    \n",
    "    DTM = pd.DataFrame()\n",
    "    for text in texts:\n",
    "        entry = convert_text_to_dtm(text)\n",
    "        DTM = DTM.append(pd.DataFrame(entry),ignore_index=True,sort=True) # Row bind\n",
    "    \n",
    "    DTM.fillna(0, inplace=True) # Fill in any missing values with 0s (i.e. when a word is in one text but not another)\n",
    "    return DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, create an accessor for that data frame, and then assign each row (1 row = 1 article) to a vector\n",
    "DTM = gen_DTM(articles_tokenized)\n",
    "aljz_vec = DTM.iloc[0].values\n",
    "bbc_vec = DTM.iloc[1].values\n",
    "bbt_vec = DTM.iloc[2].values\n",
    "cnn_vec = DTM.iloc[3].values\n",
    "fox_vec = DTM.iloc[4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The directions said not to just copy the cosine calculation function. I hope this is an acceptable reworking. \n",
    "    \n",
    "def cosine(a,b):\n",
    "    \n",
    "    '''\n",
    "    Takes two vectors and calculates the cosine of the angle between them.\n",
    "    Arguments: a = a vector\n",
    "               b = a vector\n",
    "    '''\n",
    "    a_dot = np.dot(a,a)\n",
    "    b_dot = np.dot(b,b)\n",
    "    ab_dot = np.dot(a,b)\n",
    "    cos = ab_dot/(np.sqrt(a_dot)*np.sqrt(b_dot))\n",
    "    return(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6362638437556203"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(aljz_vec, bbc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5252670250259308"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(aljz_vec, bbt_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5001486244449631"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(aljz_vec, cnn_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038829965072295"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(aljz_vec, fox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5335823775905203"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(bbc_vec, bbt_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4701307151063014"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(bbc_vec, cnn_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5747346493651461"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(bbc_vec, fox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3250974994371181"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(bbt_vec, cnn_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4955390278235966"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(bbt_vec, fox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49054506610222515"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(cnn_vec, fox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566390622433436"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just for fun, I am going to find the mean similarity for each text. This is the mean for Al-Jazeera\n",
    "(cosine(aljz_vec, bbc_vec)+cosine(aljz_vec, bbt_vec)+cosine(aljz_vec, cnn_vec)+cosine(aljz_vec, fox_vec))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553677896454397"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean for the BBC\n",
    "(cosine(aljz_vec, bbc_vec)+cosine(bbc_vec, bbt_vec)+cosine(bbc_vec, cnn_vec)+cosine(bbc_vec, fox_vec))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46987148246929145"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean for Breitbart\n",
    "(cosine(bbt_vec, bbc_vec)+cosine(aljz_vec, bbt_vec)+cosine(bbt_vec, cnn_vec)+cosine(bbt_vec, fox_vec))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44648047627265197"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean for CNN\n",
    "(cosine(cnn_vec, bbc_vec)+cosine(cnn_vec, bbt_vec)+cosine(aljz_vec, cnn_vec)+cosine(cnn_vec, fox_vec))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5411754349495493"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean for Fox\n",
    "(cosine(fox_vec, bbc_vec)+cosine(fox_vec, bbt_vec)+cosine(fox_vec, cnn_vec)+cosine(aljz_vec, fox_vec))/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "-Al-Jazeera is most similar to the BBC, and most dissimilar to CNN.\n",
    "\n",
    "-The BBC is most similar to Al-Jazeera, and most dissimilar to CNN.\n",
    "\n",
    "-Breitbart is most similar to the BBC, and most dissimilar to CNN.\n",
    "\n",
    "-CNN is most similar to Al-Jazeera, and most dissimilar to Breitbart.\n",
    "\n",
    "-Fox is most similar to Al-Jazeera, and most dissimilar to CNN.\n",
    "\n",
    "The cosine values range from a low of 0.325 between Breitbart and CNN to a high of 0.636 between Al-Jazeera and the BBC. The fact that CNN is the least similar of all of these texts suggests that the CNN reporting either 1) contains information that the other four do not, or 2) is missing information contained in the other four. I also calculated the mean similarity for each text. Al-Jazeera, the BBC, and Fox all have a mean similarity of about 0.55, while Breitbart and CNN have a mean similarity of about 0.45. I predict that expanding the list of stop words would decrease the cosine values. Adding more stop words would likely reduce the number of words in common between texts, which would decrease the dot product of the two vectors and decrease the cosine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
